# Korean translated version of 'Machine Learning Yearning'

This repository is to provide Korean translated version of 'Machine Learning Yearning' book by Andrew Ng.

<img src="./img/covers.png">

## Contents

[1 왜 머신러닝에 대한 전략을 알아야 하는가?](./1_Why_Machine_Learning_Strategy.md)

[2 이 책을 사용 하는 방법](./2_How_to_use_this_book_to_help_your_team.md)

[3 이 책을 읽기전 미리 알아야 하는 것과 사용되는 표기법](./3_Prerequisites_and_notation.md)

[4 규모가 머신러닝의 진보를 이끈다](./4_Scale_drives_machine_learning_progress.md)

[5 개발 데이터셋과 테스트 데이터셋](./5_Your_development_and_test_sets.md)

[6 개발/테스트 데이터셋은 같은 분포의 데이터로 구성되어야 한다](./6_Your_dev_and_test_sets_should_come_from_the_same_distribution.md)

[7 개발/테스트 데이터셋이 얼마나 커야 하는가?](./7_How_large_do_the_dev_test_sets_need_to_be.md)

[8 알고리즘 최적화를 위해서 단일-숫자 평가지표를 설정하는것](./8_Establish_a_single-number_evaluation_metric_for_your_team_to_optimize.md)

[9 최적화와 만족화라는 평가 지표에 대해서](./9_Optimizing_and_satisficing_metrics.md)

[10 개발 데이터셋과 평가지표로 개발 사이클 순환 속도를 빠르게](./10_Having_a_dev_set_and_metric_speeds_up_iterations.md)

[11 개발/테스트 데이터셋과 평가지표를 언제 바꿔야 하는가?](./11_When_to_change_dev_test_sets_and_metrics.md)

[12 요약: 개발 데이터셋과 테스트 데이터셋을 설정하는 것에 관하여](./12_Takeaways_Setting_up_development_and_test_sets.md)

13 Build your first system quickly, then iterate

14 Error analysis: Look at dev set examples to evaluate ideas

15 Evaluating multiple ideas in parallel during error analysis

16 Cleaning up mislabeled dev and test set examples

17 If you have a large dev set, split it into two subsets, only one of which you look at

18 How big should the Eyeball and Blackbox dev sets be?

19 Takeaways: Basic error analysis

20 Bias and Variance: The two big sources of error

21 Examples of Bias and Variance

22 Comparing to the optimal error rate

23 Addressing Bias and Variance

24 Bias vs. Variance tradeoff

25 Techniques for reducing avoidable bias

26 Error analysis on the training set

27 Techniques for reducing variance

28 Diagnosing bias and variance: Learning curves

29 Plotting training error

30 Interpreting learning curves: High bias

31 Interpreting learning curves: Other cases

32 Plotting learning curves

33 Why we compare to human-level performance

34 How to define human-level performance

35 Surpassing human-level performance

36 When you should train and test on different distributions

37 How to decide whether to use all your data

38 How to decide whether to include inconsistent data

39 Weighting data

40 Generalizing from the training set to the dev set

41 Addressing Bias and Variance

42 Addressing data mismatch

43 Artificial data synthesis

44 The Optimization Verification test

45 General form of Optimization Verification test

46 Reinforcement learning example

47 The rise of end-to-end learning

48 More end-to-end learning examples

49 Pros and cons of end-to-end learning

50 Learned sub-components

51 Directly learning rich outputs

52 Error Analysis by Parts

53 Beyond supervised learning: What’s next?

54 Building a superhero team - Get your teammates to read this

55 Big picture

56 Credits