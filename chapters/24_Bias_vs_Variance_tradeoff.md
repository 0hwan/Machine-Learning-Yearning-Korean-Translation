## 24 Bias vs. Variance tradeoff

**편향과 분산 사이의 균형 대립** 이라는 말을 들어본 적이 있는가? 학습 알고리즘에 줄 수 있는 대부분의 변화들 중에서, 편향에 대한 에러를 낮추는 것이 분산에 대한 에러를 증가시키거나, 그 반대의 경우가 발생하곤 한다. 이 상황은 ***`편향`*** 과 ***`분산`*** 사이의 ***`균형 대립`*** 이라고 설명될 수 있다.

예를 들어보자. 더 많은 유닛(뉴런) 또는 레이어를 추가하거나, 입력 feature를 더 많이 구성해서 모델의 크기를 증가시키면, 일반적으로 ***`편향`*** 을 줄여줄 수는 있으나 ***`분산`*** 은 증가하게 된다. 이에 대한 해결 대안으로, 정규화 방법의 추가가 일반적으로 ***`편향`*** 은 증가시키지만 ***`분산`*** 을 줄어들게 만든다.

충분한 데이터에 접근과 매우 큰 뉴럴넷(딥러닝)의 사용이 현대에와서 종종 가능해지고 있다. 그렇기에 **균형 대립** 문제가 비교적 덜 하고, ***`분산`*** 에 의한 문제를 일으키지 않고 ***`편향`*** 을 줄여나가거나, 그 반대의 결과를 얻을 수 있는 더 많은 방법들이 존재한다.

예를 들어서, 뉴럴넷의 크기를 증가시키고, 정규화 방법을 사용하여 ***`분산`*** 의 큰 증가 없이 ***`편향`*** 을 줄여나가는 일반적인 방법이 있을 수 있다. 마찬가지로 더 많은 학습 데이터를 추가하면, ***`편향`*** 에 영향을 안주고 ***`분산`*** 을 줄여나가는 것이 일반적으로 가능하다. 

현재 처한 문제에 적합한 특정 구조의 모델을 선택 했다면, ***`분산`*** 과 ***`편향`*** 두 가지를 동시에 줄여나갈 수 있을지도 모른다. 하지만, 그런 모델 구조를 고르는 것이란 어려운 일이다. 다음 몇 챕터에서, 편향과 분산문제를 해결하기 위한 추가적인 특정 기법에 대하여 이야기 해보고자 한다.
