## 24 Bias vs. Variance tradeoff

"편향과 분산 사이의 균형 대립" 이라는 말을 들어봤는가? 대부분의 학습 알고리즘에 줄 수 있는 변화들 중에서, 편향 에러는 줄여주지만 분산은 증가시키는 경우나 그 반대의 결과를 가져오는 케이스가 있다. 이 상황이 편향과 분산사이의 "균형 대립" 이라는 상황을 만들어낸다.

예를 들어보자. 더 많은 뉴런 또는 레이어를 추가하거나 입력 feature를 더 많이 구성해서 모델의 크기를 증가시키면, 일반적으로 편향을 줄여줄 수는 있으나 분산치는 증가하게 된다. 이에 대한 대안으로, 정규화 방법을 추가하면 일반적으로 편향을 증가시키지만 분산은 줄어들게 된다.

현대 사회에서, 충분한 데이터에 접근과 매우 큰 뉴럴넷(딥러닝)을 사용하는 것이 종종 가능해지고 있다. 그렇기에 "균형 대립" 문제가 비교적 덜하고, 분산 문제를 일으키지 않고 편향을 줄여나가거나 그 반대의 결과를 얻을 수 있는 더 많은 선택적 방버들이 존재한다.

예를 들어서, 보통 뉴럴넷의 크기를 증가시킬 수 있고, 정규화 방법을 사용하여 분산치의 큰 증가 없이 편향을 줄여나갈 수 있다. 더 마찬가지로 많은 학습 데이터를 추가하면, 일반적으로 편향에 영향을 안주고 분산을 줄여나가는 것이 가능하다. 

당신이 처한 문제에 적합한 특정 모델 구조의 선택을 했다면, 분산과 편향 두가지를 동시에 줄여나갈 수 있을지도 모른다. 그러나 그런 모델 구조를 고르는 것이란 어려운 일이다. 다음 몇가지 챕터에서, 편향과 분산문제를 해결하기 위한 추가적인 특정 기법에 대하여 이야기 해보고자 한다.